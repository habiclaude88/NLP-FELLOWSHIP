{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/habiclaude88/NLP-FELLOWSHIP/blob/main/Web_scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name: Jean Claude HABIMANA**"
      ],
      "metadata": {
        "id": "tDM-B4ARxseI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment**: **Web scrapping; Igihe.com**"
      ],
      "metadata": {
        "id": "07W3ZYr0bF39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unsortedLinks = []\n",
        "\n",
        "for month in range(1, 11):\n",
        "  for day in range(1, 32):\n",
        "    l = \"http://archive.org/wayback/available?url=igihe.com&timestamp=2022{:02d}{:02d}\".format(month, day)\n",
        "    response = requests.get(l).json()\n",
        "    if  response['archived_snapshots']:\n",
        "      unsortedLinks.append(response['archived_snapshots']['closest'][\"url\"])"
      ],
      "metadata": {
        "id": "uAeHmrCaRE9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(unsortedLinks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wazMetsxUIXE",
        "outputId": "7380ecad-579a-4438-8144-4efb97f94548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "307"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array(unsortedLinks)\n",
        "series = pd.Series(arr)\n",
        "unsortedLinks = series.unique()"
      ],
      "metadata": {
        "id": "aaObZfxtV7y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "serious_links = {}\n"
      ],
      "metadata": {
        "id": "9SVdzehDhrCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# link = \"https://web.archive.org/web/20220615035616/https://www.igihe.com/amakuru/\"\n",
        "def process_links(link):\n",
        "  page = requests.get(link)\n",
        "  soup = bs(page.text, \"html.parser\")\n",
        "  \n",
        "  b = soup.body\n",
        "  get_titles = [link+re.sub(\"amakuru/\",\"\", a['href']) for span in b.find_all(\"span\", class_ = \"homenews-title\") for a in span.find_all(\"a\")]\n",
        "  \n",
        "  # get only links whose date is in current year\n",
        "  pattern = re.compile(r\"\\d{8,}?\")\n",
        "  found_date = pattern.search(link).group()\n",
        "  if found_date[3] != str(2):\n",
        "    pass\n",
        "  else:\n",
        "    file_title = f\"igihe-{found_date[0:4]}-{found_date[4:6]}-{found_date[6:8]}\"\n",
        "    try:\n",
        "      for title in get_titles:\n",
        "        serious_links[file_title].append(title)\n",
        "    except KeyError:\n",
        "      serious_links[file_title] = []\n",
        "      for title in get_titles:\n",
        "        serious_links[file_title].append(title)"
      ],
      "metadata": {
        "id": "RWBqtAaGnqEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for link in unsortedLinks:\n",
        "  process_links(link)"
      ],
      "metadata": {
        "id": "vN79F78QK76k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "serious_links"
      ],
      "metadata": {
        "id": "-H6yZC9fMRQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writeToFile(paragraphs, file):\n",
        "  with open(file, \"a+\") as f:\n",
        "    for line in paragraphs:\n",
        "      f.write(line)"
      ],
      "metadata": {
        "id": "3tAWz2wTOIPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page():\n",
        "  count = 1\n",
        "  for key in list(serious_links.keys()):\n",
        "    for link in set(serious_links[key]):\n",
        "      p = requests.get(link).content\n",
        "      soup = bs(p, \"html.parser\")\n",
        "      spans = soup.find_all(\"div\", class_= \"fulltext margintop10\")\n",
        "      paragraphs = [p.get_text() for span in spans for p in span.find_all(\"p\")]\n",
        "      writeToFile(paragraphs = paragraphs, file = f\"/content/drive/MyDrive/NLP fellowship/week1 tasks/{key}.txt\")\n",
        "    print(f\"file {count} created\")\n",
        "    count += 1"
      ],
      "metadata": {
        "id": "modd8RmbpQNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINISH THE PROGRAM\n",
        "get_page()"
      ],
      "metadata": {
        "id": "TndWvTTCmqer"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}